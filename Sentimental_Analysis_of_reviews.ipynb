{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "VeryDeepLearningNLP_Exercise.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b6q_uQneHgHB",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# read data from reviews and labels file.\n",
        "with open('reviews.txt', 'r') as f:\n",
        "    reviews_ = f.readlines()\n",
        "with open('labels.txt', 'r') as f:\n",
        "    \n",
        "    labels = f.readlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-vkiX3ulHgHD",
        "outputId": "81f1f528-9180-4413-a1e2-c71e2791d464",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "# One of the most important task is to visualize data before starting with any ML task. \n",
        "for i in range(5):\n",
        "    print(labels[i] + \"\\t: \" + reviews_[i][:100] + \"...\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positive\n",
            "\t: bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life...\n",
            "negative\n",
            "\t: story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terr...\n",
            "positive\n",
            "\t: homelessness  or houselessness as george carlin stated  has been an issue for years but never a plan...\n",
            "negative\n",
            "\t: airport    starts as a brand new luxury    plane is loaded up with valuable paintings  such belongin...\n",
            "positive\n",
            "\t: brilliant over  acting by lesley ann warren . best dramatic hobo lady i have ever seen  and love sce...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E1CK9RYiHgHG"
      },
      "source": [
        "\n",
        "\n",
        "We can see there are a lot of punctuation marks like fullstop(.), comma(,), new line (\\n) and so on and we need to remove it. \n",
        "\n",
        "Here is a list of all the punctuation marks that needs to be removed \n",
        "```\n",
        "(!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4madaD3CHgHH",
        "colab": {}
      },
      "source": [
        "# Make everything lower case to make the whole dataset even. \n",
        "reviews = ''.join(reviews_).lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hyfx2rvyHgHJ",
        "colab": {}
      },
      "source": [
        "# complete the function below to remove punctuations and save it in no_punct_text\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "def text_without_punct_returnwords(reviews):\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    words=tokenizer.tokenize(reviews)\n",
        "    return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q8yS_sYVHgHK",
        "colab": {}
      },
      "source": [
        "# split the formatted no_punct_text into words\n",
        "words = text_without_punct_returnwords(reviews)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2t2VX4z6HgHM",
        "outputId": "cce165c1-0a12-48c2-edbe-1108c59f7df2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# once you are done print the ten words that should yield the following output\n",
        "words[:10]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', 'it', 'ran', 'at', 'the']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SPsbnwcJHgHO",
        "outputId": "0ba6cfa4-3fc1-49f6-fc34-fe443ac8e330",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# print the total length of the words\n",
        "len(words)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6020196"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EeAolsUhHgHQ",
        "outputId": "2a0c323f-b1ef-44ec-f1a0-6b26794816a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Total number of unique words\n",
        "len(set(words))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "74072"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LXmFZMV0HgHR"
      },
      "source": [
        "\n",
        "Next step is to create a vocabulary. This way every word is mapped to an integer number.\n",
        "```\n",
        "Example: 1: hello, 2: I, 3: am, 4: Robo and so on...\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bfzSug8FHgHS",
        "colab": {}
      },
      "source": [
        "# Lets create a vocab out of it\n",
        "\n",
        "# feel free to use this import \n",
        "from collections import Counter\n",
        "\n",
        "## Let's keep a count of all the words and let's see how many words are there. \n",
        "def word_count(words):\n",
        "    return Counter(words)\n",
        "\n",
        "counts=word_count(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Jk6FqxM8HgHT",
        "outputId": "3312e6a3-53b3-4027-bfde-d26f661b57fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print (counts['wonderful'])\n",
        "\n",
        "print (counts['bad'])\n",
        "counts=counts.most_common()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1658\n",
            "9308\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uc0ZTIySHgHX",
        "outputId": "4c1b5efd-c729-42e6-c0f4-4a40b531fcf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# define a vocabulary for the words\n",
        "def vocabulary(counts):\n",
        "  vocab_to_int=dict()\n",
        "  vocab=[]\n",
        "  i=1\n",
        "  for key,number in counts:\n",
        "    vocab.append(key)\n",
        "    vocab_to_int[f'{key}']=i\n",
        "    i+=1\n",
        "  return vocab_to_int,vocab\n",
        "vocab_int,vocab = vocabulary(counts)\n",
        "print(len(vocab_int))\n",
        "print(vocab[1])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "74072\n",
            "and\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_KQyLDT_HgHZ",
        "colab": {}
      },
      "source": [
        "# map each vocab word to an integer. Also, start the indexing with 1 as we will use \n",
        "# '0' for padding and we dont want to mix the two.\n",
        "vocab_to_int = vocab_int"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nue2l-krHgHb",
        "outputId": "7316c183-1ccc-46fc-bc82-1fa3d1a28531",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# verify if the length is same and if 'and' is mapped to the correct integer value.\n",
        "print(len(vocab_to_int))\n",
        "print(vocab_to_int['and'])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "74072\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LOzRK2R1HgHd"
      },
      "source": [
        "Let's see what positve words in positive reviews we have and what we have in negative reviews. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M34-j9vQHgHd",
        "colab": {}
      },
      "source": [
        "positive_counts = Counter()\n",
        "negative_counts = Counter()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1AYmrGh6HgHf",
        "colab": {}
      },
      "source": [
        "for i in range(len(reviews_)):\n",
        "    if(labels[i] == 'positive\\n'):\n",
        "        for word in reviews_[i].split(\" \"):\n",
        "            positive_counts[word] += 1\n",
        "    else:\n",
        "        for word in reviews_[i].split(\" \"):\n",
        "            negative_counts[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BetaL0zAHgHg",
        "outputId": "368e5b60-f29b-41e2-f000-89becaf5564f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "labels[:10]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h_UVAevBHgHi",
        "outputId": "c134eb7d-ca06-45f1-ebf2-2ff257344c3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "positive_counts.most_common()[:10]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('', 537968),\n",
              " ('the', 173324),\n",
              " ('.', 159654),\n",
              " ('and', 89722),\n",
              " ('a', 83688),\n",
              " ('of', 76855),\n",
              " ('to', 66746),\n",
              " ('is', 57245),\n",
              " ('in', 50215),\n",
              " ('br', 49235)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gDAvMXnWHgHj",
        "outputId": "cbe308d9-b33d-484b-edb8-bc8fd1a441bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "negative_counts.most_common()[:10]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('', 548962),\n",
              " ('.', 167538),\n",
              " ('the', 163389),\n",
              " ('a', 79321),\n",
              " ('and', 74385),\n",
              " ('of', 69009),\n",
              " ('to', 68974),\n",
              " ('br', 52637),\n",
              " ('is', 50083),\n",
              " ('it', 48327)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Zw4aYik_HgHm"
      },
      "source": [
        "The above is just to show the most common words in the positive and negative sentences. However, there are a lot of unnecessary words like `the`, `a`, `was`, and so on. Can you find a way to show the relevant words and not these words? \n",
        "\n",
        "```\n",
        "Stop Words removal or normalizing each term.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b6M8IpcrHgHn",
        "outputId": "a35c478a-5c0c-4857-eb3e-831cf08a427f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "words[:30]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bromwell',\n",
              " 'high',\n",
              " 'is',\n",
              " 'a',\n",
              " 'cartoon',\n",
              " 'comedy',\n",
              " 'it',\n",
              " 'ran',\n",
              " 'at',\n",
              " 'the',\n",
              " 'same',\n",
              " 'time',\n",
              " 'as',\n",
              " 'some',\n",
              " 'other',\n",
              " 'programs',\n",
              " 'about',\n",
              " 'school',\n",
              " 'life',\n",
              " 'such',\n",
              " 'as',\n",
              " 'teachers',\n",
              " 'my',\n",
              " 'years',\n",
              " 'in',\n",
              " 'the',\n",
              " 'teaching',\n",
              " 'profession',\n",
              " 'lead',\n",
              " 'me']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VltZZYUZHgHo",
        "outputId": "a45f19a0-0e03-48fe-c0b2-3265691fe8e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "[vocab_to_int[word] for word in words[:30]]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[21025,\n",
              " 308,\n",
              " 6,\n",
              " 3,\n",
              " 1050,\n",
              " 207,\n",
              " 8,\n",
              " 2138,\n",
              " 32,\n",
              " 1,\n",
              " 171,\n",
              " 57,\n",
              " 15,\n",
              " 49,\n",
              " 81,\n",
              " 5785,\n",
              " 44,\n",
              " 382,\n",
              " 110,\n",
              " 140,\n",
              " 15,\n",
              " 5194,\n",
              " 60,\n",
              " 154,\n",
              " 9,\n",
              " 1,\n",
              " 4975,\n",
              " 5852,\n",
              " 475,\n",
              " 71]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mHseu42RHgHq",
        "outputId": "fb317f73-16fd-431f-a95f-b4fa3f2b8e4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(vocab_to_int['bromwell'])\n",
        "import re\n",
        "no_punct_text = re.sub(r'[^\\w\\s]','',reviews)\n",
        "reviews_split = no_punct_text.split('\\n')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21025\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wBc1EvOnHgHr"
      },
      "source": [
        "## One hot encoding\n",
        "\n",
        "We need one hot encoding for the labels. Think of a reason why we need one hot encoded labels for classes?\n",
        "\n",
        "* Write the one hot encoding logic in the `one_hot` function.\n",
        "* we use 1 for positive label and 0 for negative label.\n",
        "* Save all the values in the `encoded_labels` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RDQKpGpXHgHs",
        "colab": {}
      },
      "source": [
        "# 1 for positive label and 0 for negative label\n",
        "def one_hot(labels):\n",
        "  val=[]\n",
        "  for i in labels:\n",
        "    if i=='positive\\n':\n",
        "      val.append(1)\n",
        "    else:\n",
        "      val.append(0)\n",
        "  return val\n",
        "encoded_labels = one_hot(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jpk3n17YHgHu",
        "colab": {}
      },
      "source": [
        "#print the length of your label and uncomment next line only if the encoded_labels size is 25001.\n",
        "# If you dont get the intuition behind this step, print encoded_labels to see it.\n",
        "#encoded_labels = encoded_labels[:25000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5AZeuxJAHgHv",
        "outputId": "3b2a1993-9ff4-4e7e-e674-b5684a471a27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(encoded_labels)\n",
        "print(encoded_labels[:10])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nEAZdHh1HgHy",
        "colab": {}
      },
      "source": [
        "reviews_ints = []\n",
        "for review in reviews_split:\n",
        "    reviews_ints.append([vocab_to_int[word] for word in review.split()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sxjwpzXVHgHz",
        "outputId": "d74cf1cd-01e5-401f-e181-f13c5b3c5a69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# This step is to see if any review is empty and we remove it. Otherwise the input will be all zeroes.\n",
        "review_lens = Counter([len(x) for x in reviews_ints])\n",
        "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
        "print(\"Maximum review length: {}\".format(max(review_lens)))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Zero-length reviews: 1\n",
            "Maximum review length: 2514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zpanRYOwHgH0",
        "outputId": "a6e3cd23-37b3-4d8a-80b1-6fc56a8d6ba3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print('Number of reviews before removing outliers: ', len(reviews_ints))\n",
        "\n",
        "## remove any reviews/labels with zero length from the reviews_ints list.\n",
        "\n",
        "# get indices of any reviews with length 0\n",
        "non_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review) != 0]\n",
        "\n",
        "# remove 0-length reviews and their labels\n",
        "reviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\n",
        "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\n",
        "\n",
        "print('Number of reviews after removing outliers: ', len(reviews_ints))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of reviews before removing outliers:  25001\n",
            "Number of reviews after removing outliers:  25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HWSwXpnQHgH2",
        "outputId": "fd464d77-c540-42ae-9a12-40885246de83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(encoded_labels)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XVaEN21dHgH4"
      },
      "source": [
        "## Task 4: Padding the data\n",
        "\n",
        ">A function that returns an array `features` that contains the padded data, of a standard size, that we'll pass to the network. \n",
        "* The data should come from `review_ints`, since we want to feed integers to the network. \n",
        "* Each row should be `seq_length` elements long. \n",
        "* For reviews shorter than `seq_length` words, **left pad** with 0s. That is, if the review is `['best', 'movie', 'ever']`, `[117, 18, 128]` as integers, the row will look like `[0, 0, 0, ..., 0, 117, 18, 128]`. \n",
        "* For reviews longer than `seq_length`, use only the first `seq_length` words as the feature vector.\n",
        "\n",
        "As a small example, if the `seq_length=10` and an input review is: \n",
        "```\n",
        "[117, 18, 128]\n",
        "```\n",
        "The resultant, padded sequence should be: \n",
        "\n",
        "```\n",
        "[0, 0, 0, 0, 0, 0, 0, 117, 18, 128]\n",
        "```\n",
        "\n",
        "**Your final `features` array should be a 2D array, with as many rows as there are reviews, and as many columns as the specified `seq_length`.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o-crPZMeHgH4",
        "colab": {}
      },
      "source": [
        "#  logic for padding the data\n",
        "def pad_features(reviews_ints, seq_length):\n",
        "    new_list=[]\n",
        "    for i in reviews_ints: \n",
        "        if len(i)<=seq_length:\n",
        "            new_list.append((seq_length-len(i))*[0]+i)\n",
        "        else:\n",
        "            new_list.append(i[:seq_length])\n",
        "    return np.array(new_list)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "38Ke89T_HgH6",
        "outputId": "05214f7d-091f-44fe-f5c6-b985effb0cb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        " \n",
        "\n",
        "seq_length = 200\n",
        "\n",
        "features = pad_features(reviews_ints, seq_length=seq_length)\n",
        "\n",
        "## test statements - ##\n",
        "assert len(features)==len(reviews_ints), \"Your features should have as many rows as reviews.\"\n",
        "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
        "\n",
        "# print first 10 values of the first 30 batches \n",
        "print(features[:30,:10])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [22382    42 46418    15   706 17139  3389    47    77    35]\n",
            " [ 4505   505    15     3  3342   162  8312  1652     6  4819]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   54    10    14   116    60   798   552    71   364     5]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    1   330   578    34     3   162   748  2731     9   325]\n",
            " [    9    11 10171  5305  1946   689   444    22   280   673]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    1   307 10399  2069  1565  6202  6528  3288 17946 10628]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   21   122  2069  1565   515  8181    88     6  1325  1182]\n",
            " [    1    20     6    76    40     6    58    81    95     5]\n",
            " [   54    10    84   329 26230 46427    63    10    14   614]\n",
            " [   11    20     6    30  1436 32317  3769   690 15100     6]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   40    26   109 17952  1422     9     1   327     4   125]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   10   499     1   307 10399    55    74     8    13    30]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_bAm-7gXHgH8"
      },
      "source": [
        "Now we have everything ready. It's time to split our dataset into `Train`, `Test` and `Validate`. \n",
        "\n",
        "\n",
        "##  Lets create train, test and val split in the ratio of 8:1:1.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KZENosPGHgH8",
        "colab": {}
      },
      "source": [
        "train_frac = 0.8\n",
        "val_frac = 0.1\n",
        "test_frac = 0.1\n",
        "\n",
        "\n",
        "def train_test_val_split(features):\n",
        "    train_x=features[:int(0.8*len(features))]\n",
        "    val_x=features[int(0.8*len(features)):int(0.9*len(features))]\n",
        "    test_x=features[int(0.9*len(features)):]\n",
        "    return train_x,val_x,test_x\n",
        "                          \n",
        "def train_test_val_labels(encoded_labels):\n",
        "    train_y=encoded_labels[:int(0.8*len(encoded_labels))]\n",
        "    val_y=encoded_labels[int(0.8*len(encoded_labels)):int(0.9*len(encoded_labels))]\n",
        "    test_y=encoded_labels[int(0.9*len(encoded_labels)):]\n",
        "    return np.array(train_y),np.array(val_y),np.array(test_y)\n",
        "    \n",
        "\n",
        "train_x, val_x, test_x = train_test_val_split(features)\n",
        "train_y, val_y, test_y = train_test_val_labels(encoded_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o0ck6kwuHgH9",
        "outputId": "bc55d957-a96e-4341-9dd7-c0f1d7cf10dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "## print out the shapes of your resultant feature data\n",
        "print(\"\\t\\t\\tFeature Shapes:\")\n",
        "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
        "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
        "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\t\tFeature Shapes:\n",
            "Train set: \t\t(20000, 200) \n",
            "Validation set: \t(2500, 200) \n",
            "Test set: \t\t(2500, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DZaPEltEHgH_"
      },
      "source": [
        "## DataLoaders and Batching\n",
        "\n",
        "After creating training, test, and validation data, we can create DataLoaders for this data by following two steps:\n",
        "1. Create a known format for accessing our data, using [TensorDataset](https://pytorch.org/docs/stable/data.html#) which takes in an input set of data and a target set of data with the same first dimension, and creates a dataset.\n",
        "2. Create DataLoaders and batch our training, validation, and test Tensor datasets.\n",
        "\n",
        "```\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
        "```\n",
        "\n",
        "This is an alternative to creating a generator function for batching our data into full batches.\n",
        "\n",
        "### A generator function for the dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RCArVXTFHgH_",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# create Tensor datasets for train, test and val\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "# dataloaders\n",
        "batch_size = 50 \n",
        "\n",
        "# make sure to SHUFFLE your training data. Keep Shuffle=True.\n",
        "train_loader = DataLoader(train_data,batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_data,batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data,batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BMMMghSNHgIA",
        "outputId": "5bef8f48-b4a0-4dbb-c324-9022ea254f29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "# obtain one batch of training data and label. \n",
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = dataiter.next()\n",
        "\n",
        "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
        "print('Sample input: \\n', sample_x)\n",
        "print()\n",
        "print('Sample label size: ', sample_y.size()) # batch_size\n",
        "print('Sample label: \\n', sample_y)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample input size:  torch.Size([50, 200])\n",
            "Sample input: \n",
            " tensor([[   10,    84,   329,  ...,    41,   235,     9],\n",
            "        [   11,    18,     6,  ...,     1,   870,  1264],\n",
            "        [ 1281,     1,   112,  ...,  3656, 11307,  1163],\n",
            "        ...,\n",
            "        [ 5017,    14,  6662,  ...,     4,     1,   512],\n",
            "        [   11,    18,   166,  ...,  2115,  2194,     6],\n",
            "        [    0,     0,     0,  ...,    52,    72,   560]])\n",
            "\n",
            "Sample label size:  torch.Size([50])\n",
            "Sample label: \n",
            " tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
            "        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
            "        1, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9-zMMkYBHgIC",
        "outputId": "5c640d87-7007-40a7-eea9-150d9448eb5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Check if GPU is available.\n",
        "train_on_gpu=torch.cuda.is_available()\n",
        "\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU.')\n",
        "else:\n",
        "    print('No GPU available, training on CPU.')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8XNsRuOUHgIH"
      },
      "source": [
        "\n",
        "LSTM  Model\n",
        "\n",
        "Now create a class named SentimentLSTM with `n_layers=2`, and rest all hyperparameters same as before. Also, create an embedding layer and feed the output of the embedding layer as input to the LSTM model. Dont forget to add a regularizer (dropout) layer after the LSTM layer with p=0.4 to prevent overfitting. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tovPxwcpHgII",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SentimentLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    The LSTM model that will be used to perform Sentiment analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.4):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super(SentimentLSTM, self).__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # embedding and LSTM layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        \n",
        "        # linear and sigmoid layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on some input and hidden state.\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "        \n",
        "        # embeddings and lstm_out\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "        \n",
        "        # stack up lstm outputs\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        # dropout and fully connected layer\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out)\n",
        "        \n",
        "        # reshape to be batch_size first\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "        sig_out = sig_out[:, -1] # get last batch of labels\n",
        "        \n",
        "        # return last sigmoid output and hidden state\n",
        "        return sig_out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "          hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                   weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "          hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                   weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "        \n",
        "        return hidden\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x2-vLBlTHgIK"
      },
      "source": [
        "## Instantiate the network\n",
        "\n",
        "Here, we'll instantiate the network. First up, defining the hyperparameters.\n",
        "\n",
        "* `vocab_size`: Size of our vocabulary or the range of values for our input, word tokens.\n",
        "* `output_size`: Size of our desired output; the number of class scores we want to output (pos/neg).\n",
        "* `embedding_dim`: Number of columns in the embedding lookup table; size of our embeddings.\n",
        "* `hidden_dim`: Number of units in the hidden layers of our LSTM cells. Usually larger is better performance wise. Common values are 128, 256, 512, etc.\n",
        "* `n_layers`: Number of LSTM layers in the network. Typically between 1-3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0w6PdDaNHgIK",
        "outputId": "d403a992-dd1a-4cf7-ad88-baa3eaecd104",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "# Instantiate the model with these hyperparameters\n",
        "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding + our word tokens\n",
        "output_size = 1\n",
        "embedding_dim = 400\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "\n",
        "net = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "\n",
        "\n",
        "print(net)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SentimentLSTM(\n",
            "  (embedding): Embedding(74073, 400)\n",
            "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.4)\n",
            "  (dropout): Dropout(p=0.4, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-2qsUH6WHgIN",
        "colab": {}
      },
      "source": [
        "# loss and optimization functions\n",
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bVIzTeDNHgIO"
      },
      "source": [
        "### Loss Functions\n",
        "We are using `BCELoss (Binary Cross Entropy Loss)` since we have two output classes. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l0Dpt8hCHgIP",
        "outputId": "9762c736-258d-4837-f031-caea0c0778eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "# training params\n",
        "\n",
        "epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
        "\n",
        "counter = 0\n",
        "print_every = 100\n",
        "clip=5 # gradient clipping\n",
        "\n",
        "# move model to GPU, if available\n",
        "if(train_on_gpu):\n",
        "    net.cuda()\n",
        "\n",
        "net.train()\n",
        "# train for some number of epochs\n",
        "for e in range(epochs):\n",
        "    # initialize hidden state\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    # batch loop\n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "\n",
        "        if(train_on_gpu):\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        # zero accumulated gradients\n",
        "        net.zero_grad()\n",
        "\n",
        "        # get the output from the model\n",
        "        output, h = net(inputs, h)\n",
        "\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss stats\n",
        "        if counter % print_every == 0:\n",
        "            # Get validation loss\n",
        "            val_h = net.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            net.eval()\n",
        "            for inputs, labels in valid_loader:\n",
        "\n",
        "                # Creating new variables for the hidden state, otherwise\n",
        "                # we'd backprop through the entire training history\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                if(train_on_gpu):\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                output, val_h = net(inputs, val_h)\n",
        "                val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            net.train()\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/4... Step: 100... Loss: 0.605741... Val Loss: 0.657327\n",
            "Epoch: 1/4... Step: 200... Loss: 0.568148... Val Loss: 0.570506\n",
            "Epoch: 1/4... Step: 300... Loss: 0.593185... Val Loss: 0.612960\n",
            "Epoch: 1/4... Step: 400... Loss: 0.463784... Val Loss: 0.624486\n",
            "Epoch: 2/4... Step: 500... Loss: 0.525115... Val Loss: 0.547588\n",
            "Epoch: 2/4... Step: 600... Loss: 0.437125... Val Loss: 0.488235\n",
            "Epoch: 2/4... Step: 700... Loss: 0.495678... Val Loss: 0.471928\n",
            "Epoch: 2/4... Step: 800... Loss: 0.413771... Val Loss: 0.475361\n",
            "Epoch: 3/4... Step: 900... Loss: 0.360896... Val Loss: 0.453201\n",
            "Epoch: 3/4... Step: 1000... Loss: 0.475974... Val Loss: 0.476717\n",
            "Epoch: 3/4... Step: 1100... Loss: 0.263890... Val Loss: 0.432978\n",
            "Epoch: 3/4... Step: 1200... Loss: 0.246858... Val Loss: 0.425624\n",
            "Epoch: 4/4... Step: 1300... Loss: 0.408047... Val Loss: 0.519153\n",
            "Epoch: 4/4... Step: 1400... Loss: 0.156555... Val Loss: 0.459965\n",
            "Epoch: 4/4... Step: 1500... Loss: 0.117863... Val Loss: 0.486934\n",
            "Epoch: 4/4... Step: 1600... Loss: 0.116972... Val Loss: 0.478254\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aU22PWDYHgIQ"
      },
      "source": [
        "\n",
        "Now we write a prediction function to predict the output for the test set created and calculate the accuracy of the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5xNdWUGWHgIR",
        "outputId": "412e6578-97a6-4f4a-98a3-1decc241ca58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def predict(test_loader):\n",
        "      \n",
        "  # Get test data loss and accuracy\n",
        "  test_losses = [] # track loss\n",
        "  num_correct = 0\n",
        "\n",
        "  # init hidden state\n",
        "  h = net.init_hidden(batch_size)\n",
        "\n",
        "  net.eval()\n",
        "  # iterate over test data\n",
        "  for inputs, labels in test_loader:\n",
        "\n",
        "      # Creating new variables for the hidden state, otherwise\n",
        "      # we'd backprop through the entire training history\n",
        "      h = tuple([each.data for each in h])\n",
        "\n",
        "      if(train_on_gpu):\n",
        "          inputs, labels = inputs.cuda(), labels.cuda()\n",
        "      \n",
        "      # get predicted outputs\n",
        "      output, h = net(inputs, h)\n",
        "      \n",
        "      # calculate loss\n",
        "      test_loss = criterion(output.squeeze(), labels.float())\n",
        "      test_losses.append(test_loss.item())\n",
        "      \n",
        "      # convert output probabilities to predicted class (0 or 1)\n",
        "      pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
        "      \n",
        "      # compare predictions to true label\n",
        "      correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "      correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "      num_correct += np.sum(correct)\n",
        "\n",
        "\n",
        "  # accuracy over all test data\n",
        "  test_acc = num_correct/len(test_loader.dataset)\n",
        "  print(\"Test accuracy: {:.3f}\".format(test_acc*100))\n",
        "predict(test_loader)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 80.840\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}