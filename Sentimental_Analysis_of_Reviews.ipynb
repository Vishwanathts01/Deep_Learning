{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "VeryDeepLearningNLP_edited.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "8I4ANyg6HgIS"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZgXHd48cg4b",
        "colab_type": "text"
      },
      "source": [
        "## **Sentimental Analyis of Reviews**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b6q_uQneHgHB",
        "colab": {}
      },
      "source": [
        "# In this exercise, we will import libraries when needed so that we understand the need for it. \n",
        "# However, this is a bad practice and don't get used to it.\n",
        "import numpy as np\n",
        "\n",
        "# read data from reviews and labels file.\n",
        "with open('reviews.txt', 'r') as f:\n",
        "    reviews_ = f.readlines()\n",
        "with open('labels.txt', 'r') as f:\n",
        "    \n",
        "    labels = f.readlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-vkiX3ulHgHD",
        "outputId": "cb5b8ea0-52fd-4b9c-cdcc-ca0d3c4e1970",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        }
      },
      "source": [
        "# One of the most important task is to visualize data before starting with any ML task. \n",
        "for i in range(5):\n",
        "    print(labels[i] + \"\\t: \" + reviews_[i][:100] + \"...\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positive\n",
            "\t: bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life...\n",
            "negative\n",
            "\t: story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terr...\n",
            "positive\n",
            "\t: homelessness  or houselessness as george carlin stated  has been an issue for years but never a plan...\n",
            "negative\n",
            "\t: airport    starts as a brand new luxury    plane is loaded up with valuable paintings  such belongin...\n",
            "positive\n",
            "\t: brilliant over  acting by lesley ann warren . best dramatic hobo lady i have ever seen  and love sce...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E1CK9RYiHgHG"
      },
      "source": [
        "\n",
        "\n",
        "We can see there are a lot of punctuation marks like fullstop(.), comma(,), new line (\\n) and so on and we need to remove it. \n",
        "\n",
        "Here is a list of all the punctuation marks that needs to be removed \n",
        "```\n",
        "(!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4madaD3CHgHH",
        "colab": {}
      },
      "source": [
        "# Make everything lower case to make the whole dataset even. \n",
        "reviews = ''.join(reviews_).lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hyfx2rvyHgHJ",
        "colab": {}
      },
      "source": [
        "# complete the function below to remove punctuations and save it in no_punct_text\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "def text_without_punct_returnwords(reviews):\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    words=tokenizer.tokenize(reviews)\n",
        "    return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q8yS_sYVHgHK",
        "colab": {}
      },
      "source": [
        "# split the formatted no_punct_text into words\n",
        "words = text_without_punct_returnwords(reviews)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2t2VX4z6HgHM",
        "outputId": "d3d92adb-46c9-4539-e06a-7c08f0705000",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# once you are done print the ten words that should yield the following output\n",
        "words[:10]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', 'it', 'ran', 'at', 'the']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SPsbnwcJHgHO",
        "outputId": "10c07ba3-4eee-4ae1-8624-5012d6b86cb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# print the total length of the words\n",
        "len(words)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6020196"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EeAolsUhHgHQ",
        "outputId": "8e7c78ac-72c4-4999-b513-c90c134171d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Total number of unique words\n",
        "len(set(words))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "74072"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LXmFZMV0HgHR"
      },
      "source": [
        "\n",
        "Next step is to create a vocabulary. This way every word is mapped to an integer number.\n",
        "```\n",
        "Example: 1: hello, 2: I, 3: am, 4: Robo and so on...\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bfzSug8FHgHS",
        "colab": {}
      },
      "source": [
        "# Lets create a vocab out of it\n",
        "\n",
        "# feel free to use this import \n",
        "from collections import Counter\n",
        "\n",
        "## Let's keep a count of all the words and let's see how many words are there. \n",
        "def word_count(words):\n",
        "    return Counter(words)\n",
        "\n",
        "counts=word_count(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Jk6FqxM8HgHT",
        "outputId": "c2e4917d-6754-4c1a-8339-aa53187578d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# If you did everything correct, this is what you should get as output. \n",
        "print (counts['wonderful'])\n",
        "\n",
        "print (counts['bad'])\n",
        "counts=counts.most_common()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1658\n",
            "9308\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r-bkrrmcHgHW"
      },
      "source": [
        "## Word to Integer and Integer to word\n",
        "The task is to map every word to an integer value and then vice-versa. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uc0ZTIySHgHX",
        "outputId": "a4afea20-f176-434f-9c7b-5e2c44f5045e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# define a vocabulary for the words\n",
        "def vocabulary(counts):\n",
        "  vocab_to_int=dict()\n",
        "  vocab=[]\n",
        "  i=1\n",
        "  for key,number in counts:\n",
        "    vocab.append(key)\n",
        "    vocab_to_int[f'{key}']=i\n",
        "    i+=1\n",
        "  return vocab_to_int,vocab\n",
        "vocab_int,vocab = vocabulary(counts)\n",
        "print(len(vocab_int))\n",
        "print(vocab[1])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "74072\n",
            "and\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_KQyLDT_HgHZ",
        "colab": {}
      },
      "source": [
        "# map each vocab word to an integer. Also, start the indexing with 1 as we will use \n",
        "# '0' for padding and we dont want to mix the two.\n",
        "def vocabulary_to_integer(vocab):\n",
        "    pass\n",
        "\n",
        "vocab_to_int = vocab_int"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nue2l-krHgHb",
        "outputId": "5c6e8a87-f1ba-4662-8528-7c37d717668c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# verify if the length is same and if 'and' is mapped to the correct integer value.\n",
        "print(len(vocab_to_int))\n",
        "print(vocab_to_int['and'])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "74072\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LOzRK2R1HgHd"
      },
      "source": [
        "Let's see what positve words in positive reviews we have and what we have in negative reviews. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M34-j9vQHgHd",
        "colab": {}
      },
      "source": [
        "positive_counts = Counter()\n",
        "negative_counts = Counter()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1AYmrGh6HgHf",
        "colab": {}
      },
      "source": [
        "for i in range(len(reviews_)):\n",
        "    if(labels[i] == 'positive\\n'):\n",
        "        for word in reviews_[i].split(\" \"):\n",
        "            positive_counts[word] += 1\n",
        "    else:\n",
        "        for word in reviews_[i].split(\" \"):\n",
        "            negative_counts[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BetaL0zAHgHg",
        "outputId": "2696ecc7-a140-4772-f654-4a08f738fbc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        }
      },
      "source": [
        "labels[:10]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n',\n",
              " 'positive\\n',\n",
              " 'negative\\n']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h_UVAevBHgHi",
        "outputId": "336f54cc-6097-4616-b64a-da05edc2860d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        }
      },
      "source": [
        "positive_counts.most_common()[:10]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('', 537968),\n",
              " ('the', 173324),\n",
              " ('.', 159654),\n",
              " ('and', 89722),\n",
              " ('a', 83688),\n",
              " ('of', 76855),\n",
              " ('to', 66746),\n",
              " ('is', 57245),\n",
              " ('in', 50215),\n",
              " ('br', 49235)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gDAvMXnWHgHj",
        "outputId": "469fa005-53d6-44fc-859b-d6e9b99c9c93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        }
      },
      "source": [
        "negative_counts.most_common()[:10]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('', 548962),\n",
              " ('.', 167538),\n",
              " ('the', 163389),\n",
              " ('a', 79321),\n",
              " ('and', 74385),\n",
              " ('of', 69009),\n",
              " ('to', 68974),\n",
              " ('br', 52637),\n",
              " ('is', 50083),\n",
              " ('it', 48327)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b6M8IpcrHgHn",
        "outputId": "af2ee0de-db5c-49ec-8523-b81ddf60a28e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        }
      },
      "source": [
        "words[:30]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bromwell',\n",
              " 'high',\n",
              " 'is',\n",
              " 'a',\n",
              " 'cartoon',\n",
              " 'comedy',\n",
              " 'it',\n",
              " 'ran',\n",
              " 'at',\n",
              " 'the',\n",
              " 'same',\n",
              " 'time',\n",
              " 'as',\n",
              " 'some',\n",
              " 'other',\n",
              " 'programs',\n",
              " 'about',\n",
              " 'school',\n",
              " 'life',\n",
              " 'such',\n",
              " 'as',\n",
              " 'teachers',\n",
              " 'my',\n",
              " 'years',\n",
              " 'in',\n",
              " 'the',\n",
              " 'teaching',\n",
              " 'profession',\n",
              " 'lead',\n",
              " 'me']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VltZZYUZHgHo",
        "outputId": "e5f4f145-2a48-44d8-b592-21d6844dabd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        }
      },
      "source": [
        "[vocab_to_int[word] for word in words[:30]]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[21025,\n",
              " 308,\n",
              " 6,\n",
              " 3,\n",
              " 1050,\n",
              " 207,\n",
              " 8,\n",
              " 2138,\n",
              " 32,\n",
              " 1,\n",
              " 171,\n",
              " 57,\n",
              " 15,\n",
              " 49,\n",
              " 81,\n",
              " 5785,\n",
              " 44,\n",
              " 382,\n",
              " 110,\n",
              " 140,\n",
              " 15,\n",
              " 5194,\n",
              " 60,\n",
              " 154,\n",
              " 9,\n",
              " 1,\n",
              " 4975,\n",
              " 5852,\n",
              " 475,\n",
              " 71]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mHseu42RHgHq",
        "outputId": "f9a53204-33aa-4ad2-db41-e60a6c8496d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print(vocab_to_int['bromwell'])\n",
        "import re\n",
        "no_punct_text = re.sub(r'[^\\w\\s]','',reviews)\n",
        "reviews_split = no_punct_text.split('\\n')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21025\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wBc1EvOnHgHr"
      },
      "source": [
        "## One hot encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RDQKpGpXHgHs",
        "colab": {}
      },
      "source": [
        "# 1 for positive label and 0 for negative label\n",
        "def one_hot(labels):\n",
        "  val=[]\n",
        "  for i in labels:\n",
        "    if i=='positive\\n':\n",
        "      val.append(1)\n",
        "    else:\n",
        "      val.append(0)\n",
        "  return val\n",
        "encoded_labels = one_hot(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5AZeuxJAHgHv",
        "outputId": "dc6173b4-fdd6-4d2a-8718-cfc669975090",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "len(encoded_labels)\n",
        "print(encoded_labels[:10])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nEAZdHh1HgHy",
        "colab": {}
      },
      "source": [
        "reviews_ints = []\n",
        "for review in reviews_split:\n",
        "    reviews_ints.append([vocab_to_int[word] for word in review.split()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sxjwpzXVHgHz",
        "outputId": "d76d05e3-8e8b-4796-a56a-4b549e69163b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# This step is to see if any review is empty and we remove it. Otherwise the input will be all zeroes.\n",
        "review_lens = Counter([len(x) for x in reviews_ints])\n",
        "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
        "print(\"Maximum review length: {}\".format(max(review_lens)))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Zero-length reviews: 1\n",
            "Maximum review length: 2514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zpanRYOwHgH0",
        "outputId": "ee929113-9424-4e05-a859-ab4957de55cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print('Number of reviews before removing outliers: ', len(reviews_ints))\n",
        "\n",
        "## remove any reviews/labels with zero length from the reviews_ints list.\n",
        "\n",
        "# get indices of any reviews with length 0\n",
        "non_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review) != 0]\n",
        "\n",
        "# remove 0-length reviews and their labels\n",
        "reviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\n",
        "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\n",
        "\n",
        "print('Number of reviews after removing outliers: ', len(reviews_ints))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of reviews before removing outliers:  25001\n",
            "Number of reviews after removing outliers:  25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HWSwXpnQHgH2",
        "outputId": "40965c91-2a38-4f78-acc7-33ef03f07001",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "len(encoded_labels)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o-crPZMeHgH4",
        "colab": {}
      },
      "source": [
        "# Write the logic for padding the data\n",
        "def pad_features(reviews_ints, seq_length):\n",
        "    new_list=[]\n",
        "    for i in reviews_ints: \n",
        "        if len(i)<=seq_length:\n",
        "            new_list.append((seq_length-len(i))*[0]+i)\n",
        "        else:\n",
        "            new_list.append(i[:seq_length])\n",
        "    return np.array(new_list)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "38Ke89T_HgH6",
        "outputId": "d0aee771-9b73-4e14-8c69-55047e33f52f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        }
      },
      "source": [
        "# Verify if everything till now is correct. \n",
        "\n",
        "seq_length = 200\n",
        "\n",
        "features = pad_features(reviews_ints, seq_length=seq_length)\n",
        "\n",
        "## test statements - do not change - ##\n",
        "assert len(features)==len(reviews_ints), \"Your features should have as many rows as reviews.\"\n",
        "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
        "\n",
        "# print first 10 values of the first 30 batches \n",
        "print(features[:30,:10])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [22382    42 46418    15   706 17139  3389    47    77    35]\n",
            " [ 4505   505    15     3  3342   162  8312  1652     6  4819]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   54    10    14   116    60   798   552    71   364     5]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    1   330   578    34     3   162   748  2731     9   325]\n",
            " [    9    11 10171  5305  1946   689   444    22   280   673]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    1   307 10399  2069  1565  6202  6528  3288 17946 10628]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   21   122  2069  1565   515  8181    88     6  1325  1182]\n",
            " [    1    20     6    76    40     6    58    81    95     5]\n",
            " [   54    10    84   329 26230 46427    63    10    14   614]\n",
            " [   11    20     6    30  1436 32317  3769   690 15100     6]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   40    26   109 17952  1422     9     1   327     4   125]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   10   499     1   307 10399    55    74     8    13    30]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_bAm-7gXHgH8"
      },
      "source": [
        "Now we have everything ready. It's time to split our dataset into `Train`, `Test` and `Validate`. \n",
        "\n",
        "## Lets create train, test and val split in the ratio of 8:1:1.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KZENosPGHgH8",
        "colab": {}
      },
      "source": [
        "train_frac = 0.8\n",
        "val_frac = 0.1\n",
        "test_frac = 0.1\n",
        "\n",
        "\n",
        "def train_test_val_split(features):\n",
        "    train_x=features[:int(0.8*len(features))]\n",
        "    val_x=features[int(0.8*len(features)):int(0.9*len(features))]\n",
        "    test_x=features[int(0.9*len(features)):]\n",
        "    return train_x,val_x,test_x\n",
        "                          \n",
        "def train_test_val_labels(encoded_labels):\n",
        "    train_y=encoded_labels[:int(0.8*len(encoded_labels))]\n",
        "    val_y=encoded_labels[int(0.8*len(encoded_labels)):int(0.9*len(encoded_labels))]\n",
        "    test_y=encoded_labels[int(0.9*len(encoded_labels)):]\n",
        "    return np.array(train_y),np.array(val_y),np.array(test_y)\n",
        "    \n",
        "\n",
        "train_x, val_x, test_x = train_test_val_split(features)\n",
        "train_y, val_y, test_y = train_test_val_labels(encoded_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o0ck6kwuHgH9",
        "outputId": "88f03a1b-8adc-410c-cd30-fbc6d8ca35a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "## print out the shapes of your resultant feature data\n",
        "print(\"\\t\\t\\tFeature Shapes:\")\n",
        "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
        "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
        "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\t\tFeature Shapes:\n",
            "Train set: \t\t(20000, 200) \n",
            "Validation set: \t(2500, 200) \n",
            "Test set: \t\t(2500, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DZaPEltEHgH_"
      },
      "source": [
        "## DataLoaders and Batching\n",
        "\n",
        "After creating training, test, and validation data, we can create DataLoaders for this data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RCArVXTFHgH_",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# create Tensor datasets for train, test and val\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "# dataloaders\n",
        "batch_size = 50 \n",
        "\n",
        "# make sure to SHUFFLE your training data. Keep Shuffle=True.\n",
        "train_loader = DataLoader(train_data,batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_data,batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data,batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BMMMghSNHgIA",
        "outputId": "abe2e53d-960c-4a5c-a634-5f012ba79fe1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "source": [
        "# obtain one batch of training data and label. \n",
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = dataiter.next()\n",
        "\n",
        "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
        "print('Sample input: \\n', sample_x)\n",
        "print()\n",
        "print('Sample label size: ', sample_y.size()) # batch_size\n",
        "print('Sample label: \\n', sample_y)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample input size:  torch.Size([50, 200])\n",
            "Sample input: \n",
            " tensor([[    0,     0,     0,  ...,    35, 10240,    18],\n",
            "        [    0,     0,     0,  ...,   403,    71,   438],\n",
            "        [    0,     0,     0,  ...,    55,   724,    20],\n",
            "        ...,\n",
            "        [    0,     0,     0,  ...,   411,     9, 30667],\n",
            "        [   11,     6,   238,  ...,   725,     9,     3],\n",
            "        [   10,    43,   329,  ...,     7,     7,    10]])\n",
            "\n",
            "Sample label size:  torch.Size([50])\n",
            "Sample label: \n",
            " tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
            "        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
            "        1, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9-zMMkYBHgIC",
        "outputId": "073a9c8d-94b5-4f1c-b319-45349ff9d75d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Check if GPU is available.\n",
        "train_on_gpu=torch.cuda.is_available()\n",
        "\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU.')\n",
        "else:\n",
        "    print('No GPU available, training on CPU.')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8XNsRuOUHgIH"
      },
      "source": [
        "\n",
        "## LSTM \n",
        "\n",
        "Before we start creating the LSTM, it is important to understand LSTM and to know why we prefer LSTM over a Vanilla RNN for this task. \n",
        "\n",
        "Now create a class named SentimentLSTM with `n_layers=2`, and rest all hyperparameters same as before. Also, create an embedding layer and feed the output of the embedding layer as input to the LSTM model. Dont forget to add a regularizer (dropout) layer after the LSTM layer with p=0.4 to prevent overfitting. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tovPxwcpHgII",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SentimentLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    The LSTM model that will be used to perform Sentiment analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super(SentimentLSTM, self).__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # embedding and LSTM layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "        # linear and sigmoid layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on some input and hidden state.\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "        \n",
        "        # embeddings and lstm_out\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "        \n",
        "        # stack up lstm outputs\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        # dropout and fully connected layer\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out)\n",
        "        \n",
        "        # reshape to be batch_size first\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "        sig_out = sig_out[:, -1] # get last batch of labels\n",
        "        \n",
        "        # return last sigmoid output and hidden state\n",
        "        return sig_out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "          hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                   weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "          hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                   weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "        \n",
        "        return hidden\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x2-vLBlTHgIK"
      },
      "source": [
        "## Instantiate the network\n",
        "\n",
        "Here, we'll instantiate the network. First up, defining the hyperparameters.\n",
        "\n",
        "* `vocab_size`: Size of our vocabulary or the range of values for our input, word tokens.\n",
        "* `output_size`: Size of our desired output; the number of class scores we want to output (pos/neg).\n",
        "* `embedding_dim`: Number of columns in the embedding lookup table; size of our embeddings.\n",
        "* `hidden_dim`: Number of units in the hidden layers of our LSTM cells. Usually larger is better performance wise. Common values are 128, 256, 512, etc.\n",
        "* `n_layers`: Number of LSTM layers in the network. Typically between 1-3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0w6PdDaNHgIK",
        "outputId": "8eeccb8e-019c-4e21-a718-2e176130b340",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "# Instantiate the model with these hyperparameters\n",
        "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding + our word tokens\n",
        "output_size = 1\n",
        "embedding_dim = 400\n",
        "hidden_dim = 256\n",
        "n_layers = 3\n",
        "\n",
        "net = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "\n",
        "\n",
        "print(net)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SentimentLSTM(\n",
            "  (embedding): Embedding(74073, 400)\n",
            "  (lstm): LSTM(400, 256, num_layers=3, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-2qsUH6WHgIN",
        "colab": {}
      },
      "source": [
        "# loss and optimization functions\n",
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bVIzTeDNHgIO"
      },
      "source": [
        "### Loss Functions\n",
        "We are using `BCELoss (Binary Cross Entropy Loss)` since we have two output classes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l0Dpt8hCHgIP",
        "outputId": "a5871182-215f-402d-b333-d85fbca539db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "# training params\n",
        "\n",
        "epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
        "\n",
        "counter = 0\n",
        "print_every = 100\n",
        "clip=5 # gradient clipping\n",
        "\n",
        "# move model to GPU, if available\n",
        "if(train_on_gpu):\n",
        "    net.cuda()\n",
        "\n",
        "net.train()\n",
        "# train for some number of epochs\n",
        "for e in range(epochs):\n",
        "    # initialize hidden state\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    # batch loop\n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "\n",
        "        if(train_on_gpu):\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        # zero accumulated gradients\n",
        "        net.zero_grad()\n",
        "\n",
        "        # get the output from the model\n",
        "        output, h = net(inputs, h)\n",
        "\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss stats\n",
        "        if counter % print_every == 0:\n",
        "            # Get validation loss\n",
        "            val_h = net.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            net.eval()\n",
        "            for inputs, labels in valid_loader:\n",
        "\n",
        "                # Creating new variables for the hidden state, otherwise\n",
        "                # we'd backprop through the entire training history\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                if(train_on_gpu):\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                output, val_h = net(inputs, val_h)\n",
        "                val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            net.train()\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/4... Step: 100... Loss: 0.708976... Val Loss: 0.639914\n",
            "Epoch: 1/4... Step: 200... Loss: 0.676425... Val Loss: 0.694237\n",
            "Epoch: 1/4... Step: 300... Loss: 0.704203... Val Loss: 0.693251\n",
            "Epoch: 1/4... Step: 400... Loss: 0.696971... Val Loss: 0.693216\n",
            "Epoch: 2/4... Step: 500... Loss: 0.687151... Val Loss: 0.691807\n",
            "Epoch: 2/4... Step: 600... Loss: 0.632632... Val Loss: 0.625379\n",
            "Epoch: 2/4... Step: 700... Loss: 0.595415... Val Loss: 0.548839\n",
            "Epoch: 2/4... Step: 800... Loss: 0.426603... Val Loss: 0.485608\n",
            "Epoch: 3/4... Step: 900... Loss: 0.334111... Val Loss: 0.468235\n",
            "Epoch: 3/4... Step: 1000... Loss: 0.326807... Val Loss: 0.458527\n",
            "Epoch: 3/4... Step: 1100... Loss: 0.310368... Val Loss: 0.449432\n",
            "Epoch: 3/4... Step: 1200... Loss: 0.366668... Val Loss: 0.427347\n",
            "Epoch: 4/4... Step: 1300... Loss: 0.334551... Val Loss: 0.482709\n",
            "Epoch: 4/4... Step: 1400... Loss: 0.265721... Val Loss: 0.504213\n",
            "Epoch: 4/4... Step: 1500... Loss: 0.276950... Val Loss: 0.470481\n",
            "Epoch: 4/4... Step: 1600... Loss: 0.270837... Val Loss: 0.446111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aU22PWDYHgIQ"
      },
      "source": [
        "\n",
        "### Prediction Function\n",
        "Now write a prediction function to predict the output for the test set created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5xNdWUGWHgIR",
        "outputId": "71e66429-73cd-4df2-9078-24c537689b92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "def predict(test_loader):\n",
        "      \n",
        "  # Get test data loss and accuracy\n",
        "  test_losses = [] # track loss\n",
        "  num_correct = 0\n",
        "\n",
        "  # init hidden state\n",
        "  h = net.init_hidden(batch_size)\n",
        "\n",
        "  net.eval()\n",
        "  # iterate over test data\n",
        "  for inputs, labels in test_loader:\n",
        "\n",
        "      # Creating new variables for the hidden state, otherwise\n",
        "      # we'd backprop through the entire training history\n",
        "      h = tuple([each.data for each in h])\n",
        "\n",
        "      if(train_on_gpu):\n",
        "          inputs, labels = inputs.cuda(), labels.cuda()\n",
        "      \n",
        "      # get predicted outputs\n",
        "      output, h = net(inputs, h)\n",
        "      \n",
        "      # calculate loss\n",
        "      test_loss = criterion(output.squeeze(), labels.float())\n",
        "      test_losses.append(test_loss.item())\n",
        "      \n",
        "      # convert output probabilities to predicted class (0 or 1)\n",
        "      pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
        "      \n",
        "      # compare predictions to true label\n",
        "      correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "      correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "      num_correct += np.sum(correct)\n",
        "\n",
        "\n",
        "  # accuracy over all test data\n",
        "  test_acc = num_correct/len(test_loader.dataset)\n",
        "  print(\"Test accuracy: {:.3f}\".format(test_acc*100))\n",
        "predict(test_loader)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 81.120\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}